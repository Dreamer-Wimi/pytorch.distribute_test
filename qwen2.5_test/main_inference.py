import os
import torch
import torch.distributed as dist
from transformers import AutoConfig, AutoTokenizer
from tp_model import TPQwenForCausalLM
from load_tp_weights import load_and_shard_weights

def setup_distributed():
    rank = int(os.environ['RANK'])
    world_size = int(os.environ['WORLD_SIZE'])
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    dist.init_process_group(backend='nccl', init_method='env://') # æ¨èä½¿ç”¨ nccl åš GPU é€šä¿¡
    torch.cuda.set_device(local_rank)
    return rank, world_size, local_rank

def main():
    rank, world_size, local_rank = setup_distributed()
    device = f'cuda:{local_rank}'
    # è¯·ä¿®æ”¹ä¸ºä½ çš„å®é™…è·¯å¾„
    model_path = "./qwen2.5-0.5b" 
    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)

    # åˆå§‹åŒ–æ¨¡å‹
    with torch.device("meta"): # ä½¿ç”¨ meta device åŠ é€Ÿåˆå§‹åŒ–ï¼Œé¿å…æ˜¾å­˜å ç”¨
        pass 
        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œè¿˜æ˜¯ç›´æ¥å®ä¾‹åŒ–åˆ° CPU æˆ– CUDA
        # å¦‚æœæ¨¡å‹å¾ˆå¤§ï¼Œå»ºè®®å…ˆåœ¨ meta ä¸Šåˆå§‹åŒ–ç»“æ„ï¼Œå† load_state_dict

    model = TPQwenForCausalLM(config, world_size=world_size, rank=rank).to(device)

    # åŠ è½½æƒé‡
    model = load_and_shard_weights(model_path, model, world_size, rank)
    model.eval()

    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

    # === æ•°æ®å‡†å¤‡ ===
    if rank == 0:
        prompt = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
        inputs = tokenizer(prompt, return_tensors="pt")
        input_ids = inputs.input_ids.to(device)
    else:
        input_ids = None

    # å¹¿æ’­ input_ids
    # 1. å¹¿æ’­é•¿åº¦
    if rank == 0:
        length_tensor = torch.tensor([input_ids.shape[1]], dtype=torch.long, device=device)
    else:
        length_tensor = torch.tensor([0], dtype=torch.long, device=device)
    dist.broadcast(length_tensor, src=0)

    # 2. å¹¿æ’­å†…å®¹
    seq_len = length_tensor.item()
    if rank != 0:
        input_ids = torch.zeros((1, seq_len), dtype=torch.long, device=device)
    dist.broadcast(input_ids, src=0)

    # === ç”Ÿæˆå¾ªç¯ ===
    max_new_tokens = 2
    generated_ids = []

    # Causal Mask (Prefill é˜¶æ®µéœ€è¦)
    # è¿™é‡Œçš„ Mask éœ€è¦å¤„ç†æˆåŠ æ³• Mask (0 for keep, -inf for mask)
    # Qwen2 çš„å®ç°é€šå¸¸ç”± attention å†…éƒ¨å¤„ç†ï¼Œä½†æ‰‹åŠ¨ä¼ å…¥æ›´ç¨³å¦¥
    # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬è¿™é‡Œä¼  Noneï¼Œè®© torch.matmul çš„ causality ä¾èµ–å®ç°
    # åœ¨æ ‡å‡†çš„ SDPA æˆ–æ‰‹åŠ¨ Attention ä¸­ï¼Œéœ€è¦é€šè¿‡ mask å±è”½ä¸Šä¸‰è§’

    # ç®€å•æ„é€ ä¸€ä¸ª causal mask (ä»…ç”¨äº prefill)
    attention_mask = torch.full((1, 1, seq_len, seq_len), float("-inf"), device=device)
    attention_mask = torch.triu(attention_mask, diagonal=1)

    # === ğŸ”¥ æ·»åŠ  Profilerï¼šä»è¿™é‡Œå¼€å§‹ ===
    trace_dir = f"./trace_rank{rank}"
    os.makedirs(trace_dir, exist_ok=True)

    with torch.profiler.profile(
        activities=[
            torch.profiler.ProfilerActivity.CPU,
            torch.profiler.ProfilerActivity.CUDA,
        ],
        schedule=torch.profiler.schedule(wait=0, warmup=0, active=1, repeat=1),
        on_trace_ready=torch.profiler.tensorboard_trace_handler(trace_dir),
        record_shapes=True,
        profile_memory=False,  # å¯è®¾ä¸º Trueï¼Œä½†ä¼šå¢å¤§æ–‡ä»¶
        with_stack=False,      # è®¾ä¸º True å¯çœ‹è°ƒç”¨æ ˆï¼ˆå¢å¤§æ–‡ä»¶ï¼‰
        with_flops=False,
    ) as prof:
        with torch.no_grad():
            # --- 1. Prefill ---
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask, # ä¼ å…¥ Mask
                use_cache=True
            )
            logits = outputs[0]
            past_key_values = outputs[1]
            
            # è´ªå©ªé‡‡æ ·
            next_token = torch.argmax(logits[:, -1, :], dim=-1)
            generated_ids.append(next_token.item())
            
            # --- 2. Decoding ---
            input_ids = next_token.unsqueeze(0) # [1, 1]
            
            for _ in range(max_new_tokens - 1):
                # Decoding é˜¶æ®µä¸éœ€è¦ mask (å› ä¸ºåªçœ‹ past_kv å’Œå½“å‰ token)
                outputs = model(
                    input_ids=input_ids,
                    past_key_values=past_key_values,
                    use_cache=True
                )
                logits = outputs[0]
                past_key_values = outputs[1]
                
                next_token = torch.argmax(logits[:, -1, :], dim=-1)
                
                # å¿…é¡»ä» Rank 0 å¹¿æ’­ç»“æœï¼Œä¿è¯æ‰€æœ‰å¡ä¸Šçš„è¾“å…¥ä¸€è‡´
                # è™½ç„¶ç†è®ºä¸Š TP åº”è¯¥ç®—å‡ºæ¥ä¸€æ ·ï¼Œä½†æµ®ç‚¹è¯¯å·®å¯èƒ½å¯¼è‡´æ¼‚ç§»
                dist.broadcast(next_token, src=0)
                
                generated_ids.append(next_token.item())
                input_ids = next_token.unsqueeze(0)
            
            # ç¡®ä¿æ‰€æœ‰ GPU æ“ä½œå®Œæˆ
            torch.cuda.synchronize()
        prof.step()  # è§¦å‘ä¿å­˜ tracing æ–‡ä»¶
    # === ğŸ”¥ Profiler ç»“æŸ ===

    if rank == 0:
        output_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
        print(f"Input: {prompt}")
        print(f"Output: {output_text}")
        print(f"âœ… Tracing file saved to {trace_dir}/")

    dist.destroy_process_group()

if __name__ == "__main__":
    main()